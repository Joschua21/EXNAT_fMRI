{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Script for Computing Surprisal Scores for the EXNAT studies using GPT-2**\n",
        "\n",
        "# Author: Merle Schuckart\n",
        "# Version: May 2023\n",
        "\n",
        "### Settings\n",
        "\n"
      ],
      "metadata": {
        "id": "UXl4rdHkPp1q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IqeDUAUNy3XO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# (disable %%capture to print output in console)\n",
        "\n",
        "import math # for mathematical functions\n",
        "import numpy as np # also for mathematical functions\n",
        "import pandas as pd # for dfs\n",
        "import csv # for reading in csv properly\n",
        "from google.colab import files # for downloading files\n",
        "\n",
        "# ML modules:\n",
        "!pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "# use AutoModelForCausalLM instead of AutoModelWithLMHead because AutoModelWithLMHead will be deprecated soon-ish?\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & Customize Model"
      ],
      "metadata": {
        "id": "F_pBfStGghXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# (disable %%capture to print output in console)\n",
        "\n",
        "# download pre-trained German GPT-2 model & tokenizer from the Hugging Face model hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/german-gpt2\")\n",
        "\n",
        "# initialise the model, use end-of-sequence (EOS) token as padding tokens\n",
        "model = AutoModelForCausalLM.from_pretrained(\"dbmdz/german-gpt2\", pad_token_id = tokenizer.eos_token_id)\n"
      ],
      "metadata": {
        "id": "kCNlzDv5fCfp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Demo:\n",
        "Mask tokens by setting their attention weights to 0 on all layers\n"
      ],
      "metadata": {
        "id": "kdsSO5nOiM3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the input text and the number of words to mask\n",
        "input_text = [\"Orlando\", \"liebte\", \"von\", \"Natur\", \"aus\", \"einsame\", \"Orte,\", \"weite\", \"Ausblicke\", \"und\", \"das\", \"Gefühl,\", \"für\", \"immer\", \"und\", \"ewig\"] # allein zu sein.\n",
        "n = 5  # I want to mask the last n words\n",
        "\n",
        "# get the last n words and the number of tokens to mask\n",
        "masked_words = input_text[-n:]\n",
        "masked_tokens = tokenizer.tokenize(\" \".join(masked_words))\n",
        "print(\"masking the following\", n, \"words now:\", masked_words)\n",
        "\n",
        "\n",
        "# 1 word can consist of many tokens, so get last n words\n",
        "# and tokenize them so we know how many tokens we have to mask:\n",
        "masked_words = input_text[-n:]\n",
        "n_tokens = len(tokenizer.tokenize(\" \".join(masked_words)))\n",
        "print(\" \".join(masked_words))\n",
        "\n",
        "\n",
        "# encode the full input text\n",
        "encoded_input = tokenizer(\" \".join(input_text), return_tensors=\"pt\")\n",
        "# get attention mask\n",
        "curr_attention_mask = encoded_input.attention_mask.clone()\n",
        "\n",
        "# modify the original attention mask: set weights to 0 where the input should be masked\n",
        "# --> I think if we set this attention mask in the .generate() call, we set this for all layers.\n",
        "curr_attention_mask[:, -(n_tokens):] = 0\n",
        "print(\"Attention mask: \", curr_attention_mask)\n",
        "\n",
        "# generate prediction, but use modified attention mask:\n",
        "output = model.generate(encoded_input['input_ids'],\n",
        "                        attention_mask = curr_attention_mask,\n",
        "                        max_new_tokens = 10) # generate 10 additional tokens\n",
        "# decode prediction\n",
        "predicted_text = tokenizer.decode(output[0], skip_special_tokens = True)\n",
        "print(\"Predicted text: \", predicted_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFp0J7YyZpbn",
        "outputId": "11d55c69-fb9b-46c8-f9f7-e5227c3919d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masking the following 5 words now: ['Gefühl,', 'für', 'immer', 'und', 'ewig']\n",
            "Gefühl, für immer und ewig\n",
            "Attention mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
            "Predicted text:  Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig die Natur.\n",
            "Er war ein großer Künstler,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Texts\n",
        "\n",
        "The csv contains 4 columns:\n",
        "\n",
        "word_punct = single words with punctuation\n",
        "\n",
        "word_no_punct = single words without punctuation (exception: dashes between 2 words)\n",
        "\n",
        "text_nr = Number of text, text_01 - text_09\n",
        "\n",
        "trial_nr = Index of word in the text, starting with 1. There are 300 words (aka trials) in each text.\n",
        "________\n",
        "Hint: Make sure you upload the csv in Google Colab before trying to read it in. You can upload the csv by clicking the file icon in the side bar."
      ],
      "metadata": {
        "id": "RbhESgnzgyqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in csv with texts\n",
        "texts_df = pd.read_csv('/content/Texts_surprisal_scores.csv',\n",
        "                       sep = \",\")#,\n",
        "                       #quoting = csv.QUOTE_NONE, # don't put quotes around fields\n",
        "                       #quotechar = None) # no specific character used for enclosing fields with special characters\n",
        "\n",
        "# Check dataframe - show all rows and columns in console:\n",
        "#pd.set_option('display.max_rows', None)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "#print(texts_df[\"word_punct\"])\n",
        "\n",
        "# get unique text numbers in texts_df\n",
        "text_nrs = list(set(texts_df[\"text_nr\"]))\n",
        "\n",
        "# if you want to get only a subset of the texts, uncomment this line and\n",
        "# add all texts you want to get surprisal scores for:\n",
        "#text_nrs = ['text_10']\n",
        "\n",
        "print(\"preparing surprisal scores for the following texts:\")\n",
        "print(text_nrs)"
      ],
      "metadata": {
        "id": "9QAIUBIcfj-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc8300e-1971-4a7a-d3c5-c91164c462d6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preparing surprisal scores for the following texts:\n",
            "['text_10']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Compute Surprisal Scores for each Word\n",
        "\n",
        "Loop texts in Text_surprisal_scres.csv. For each word, get chunk with x previous words and calculate probability for predicting the actual word in the text.\n",
        "\n",
        "If there are lower time scales (i.e. you have time scales with shorter context chunks), mask the already known part of the input.\n",
        "_______\n",
        "####Example:\n",
        "This is our whole sentence:\n",
        "\n",
        "\"Sphinx of black quartz, judge my vow.\"\n",
        "\n",
        "We want to get a surprisal score for the last word \"vow\" on time scales 2 and 4. To do so, we have to predict the last word, given the previous n words (with n being 2 for time scale 2 and 4 for time scale 4).\n",
        "\n",
        "So for time scale 2, our context chunk would be: \"judge my\"\n",
        "For time scale 4, the context chunk would be the last 4 words: \"black quartz, judge my\"\n",
        "\n",
        "As you can see, we have redundant information here: The last 2 words are used to generate predictions on both time scales, so in a way, time scale 4 contains time scale 2. Not ideal.\n",
        "\n",
        "Now we have 2 options:\n",
        "\n",
        "1. We could just leave out the last 2 words for time scale 4 and just use \"black quartz,\" as an input. There are 2 problems with this: a) The model doesn't \"know\" there is something missing and it has to predict not the next word, but the third next word, and b) the surprisal score on TS 4 for the current word at index n would be the same as the surprisal score on time scale 2 for the word at index n-2, which is also a bit weird.\n",
        "\n",
        "2. We could mask the redundant information. To do so, we set the attention weights for the redundant words to 0, which means they are still in the context chunk, but they are not providing information for the prediction of the next word. This is what I did here.\n",
        "\n"
      ],
      "metadata": {
        "id": "om3_-bvriKm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Compute surprisal scores with different context chunk sizes \"\"\"\n",
        "# (running this takes a few hours)\n",
        "\n",
        "# define your time scales (aka context chunk lengths) here\n",
        "time_scale = [1, 4, 12, 60]\n",
        "\n",
        "# Prepare list for collecting surprisal scores on each time scale\n",
        "for i in range(len(time_scale)):\n",
        "    # create list called surprisal_n with n being\n",
        "    # one of the 9 time scales defined above\n",
        "    exec(f\"surprisal_{time_scale[i]} = []\")\n",
        "# test if it worked: Should return an empty list\n",
        "#print(surprisal_1)\n",
        "\n",
        "# get unique text numbers in texts_df\n",
        "#text_nrs = list(set(texts_df[\"text_nr\"]))\n",
        "\n",
        "print(\"preparing surprisal scores for the following texts:\")\n",
        "print(text_nrs)\n",
        "\n",
        "\n",
        "\"\"\" Loop time scales \"\"\"\n",
        "for curr_ts_idx, curr_ts in enumerate(time_scale):\n",
        "\n",
        "  print(\"\\n\\n\\nStarting to compute surprisal scores for TS\", curr_ts)\n",
        "\n",
        "  # get correct surprisal score list for current TS\n",
        "  curr_TS_list = eval(f\"surprisal_{curr_ts}\")\n",
        "\n",
        "  # Set number of words to mask in the context chunks\n",
        "  # this depends on the length of the last TS (e.g. if the current TS\n",
        "  # is 64 and the last one is 32, we have context chunks\n",
        "  # of 64 words with the last 32 words being masked)\n",
        "  # n is the number of words that should be masked here.\n",
        "\n",
        "  # If the current TS is the first one, there is no context\n",
        "  # overlap with previous TSs (because there aren't any),\n",
        "  # so don't mask any of the previous words\n",
        "  if curr_ts_idx == 0:\n",
        "    n = 0\n",
        "  # if there are previous TSs though, mask as many words as there\n",
        "  # were words in the previous TS's context chunk.\n",
        "  else:\n",
        "    n = time_scale[curr_ts_idx-1]\n",
        "\n",
        "  # Hint: As each word consists of multiple tokens, we have to\n",
        "  # find out how many tokens have to be masked for each new\n",
        "  # context chunk. This will be done later in this script.\n",
        "\n",
        "  \"\"\" Loop texts \"\"\"\n",
        "  for text_nr in text_nrs:\n",
        "    # get subset of df with current text\n",
        "    curr_text = texts_df[texts_df[\"text_nr\"] == text_nr]\n",
        "    print(text_nr)\n",
        "\n",
        "    # The size of the context chunks equals the time scale, e.g. for TS 16\n",
        "    # we have a context chunk of the 16 words before the word we want to predict.\n",
        "    # As we always need a certain context size, the first few words from each text\n",
        "    # don't have the necessary context chunk length and consequently can't get surprisal scores.\n",
        "    # So assign None values instead, so all surprisal score lists have the same length.\n",
        "    curr_TS_list.extend([None] * curr_ts)\n",
        "\n",
        "    \"\"\" Loop words & calculate surprisal score for each one \"\"\"\n",
        "\n",
        "    # loop words, start at word with index = TS\n",
        "    # (e.g. for TS = 64, start with the 65th word)\n",
        "    for word_idx in range(curr_ts, len(curr_text[\"word_punct\"])):\n",
        "\n",
        "        \"\"\" prepare context chunk \"\"\"\n",
        "        # get n previous words (with n = TS, e.g. 64 for TS 64)\n",
        "        previous_words = list(curr_text[\"word_punct\"])[word_idx - curr_ts : word_idx]\n",
        "        #print(\"previous words\", previous_words)\n",
        "\n",
        "        # generate token ids for each of the x previous words\n",
        "        encoded_input = tokenizer(\" \".join(previous_words), # input chunk as 1 text string\n",
        "                                  add_special_tokens = False, # don't add special tokens in the output\n",
        "                                  return_tensors = \"pt\") # return as tensor object\n",
        "\n",
        "        # get token IDs, put them in a list and in a tensor\n",
        "        ids_tensor = encoded_input['input_ids']\n",
        "        ids_list = ids_tensor[0].tolist()\n",
        "\n",
        "        # get original attention mask\n",
        "        curr_attention_mask = encoded_input.attention_mask.clone()\n",
        "\n",
        "        # check which TS index we have here: If it's the\n",
        "        # first TS, we don't have to mask anything.\n",
        "        if n == 0:\n",
        "          print(\"first TS - not masking any words!\")\n",
        "          unmasked_tokens = tokenizer.tokenize(\" \".join(previous_words))\n",
        "          print(\"unmasked tokens:\", unmasked_tokens)\n",
        "\n",
        "        # if there are previous TSs though, mask as many words as there\n",
        "        # were words in the previous TS's context chunk.\n",
        "        else:\n",
        "          unmasked_words = previous_words[:-n]\n",
        "          unmasked_tokens = tokenizer.tokenize(\" \".join(unmasked_words))\n",
        "          print(\"unmasked tokens:\", unmasked_tokens)\n",
        "\n",
        "          # get the last n words and the number of tokens to mask\n",
        "          masked_words = previous_words[-n:]\n",
        "          masked_tokens = tokenizer.tokenize(\" \".join(masked_words))\n",
        "          n_tokens = len(masked_tokens)\n",
        "          print(\"masking the following\", n, \"words now:\", masked_words, \" - list of masked tokens:\", masked_tokens)\n",
        "\n",
        "          # modify the original attention mask: set weights to 0 where the input should be masked\n",
        "          # --> I think if we set this attention mask in the .generate() call, we set this for all layers.\n",
        "          curr_attention_mask[:, -n_tokens:] = 0\n",
        "\n",
        "        print(\"attention mask: \", curr_attention_mask)\n",
        "\n",
        "        \"\"\" prepare actual next word \"\"\"\n",
        "        # We should also predict punctuation.\n",
        "        # It's not like the words are shown without punctuation on screen.\n",
        "        actual_word = list(curr_text[\"word_punct\"])[word_idx]\n",
        "        print(\"actual word:\", actual_word)\n",
        "\n",
        "        # Problem: actual word might have multiple token IDs\n",
        "        # --> get all IDs for current word\n",
        "        act_word_id = tokenizer.encode(actual_word)\n",
        "        # Output looks somewhat like this: [44, 305, 479, 5283]\n",
        "\n",
        "        \"\"\" loop IDs of current word \"\"\"\n",
        "        curr_id_probs = []\n",
        "        for curr_id in act_word_id:\n",
        "\n",
        "            print(\"computing probability for ID \" +  str(curr_id) + \" of word \" + actual_word)\n",
        "\n",
        "            # generate probabilities for each possible token being the actual next token\n",
        "            # --> use modified attention mask:\n",
        "            output = model.generate(ids_tensor,\n",
        "                                    attention_mask = curr_attention_mask,\n",
        "                                    return_dict_in_generate = True,\n",
        "                                    output_scores = True,\n",
        "                                    max_new_tokens = 1) # set output length here - 1 because I only want 1 token\n",
        "\n",
        "            # read out probabilities for all IDs\n",
        "            logits = output.scores[0] # logits = probabilities with range [0,1] transformed to range [inf, -inf]\n",
        "            probs = tf.nn.softmax(logits) # transform logits back to probabilities\n",
        "\n",
        "            # get probability for actual ID being the next one & append it to\n",
        "            # array with probabilities of all IDs for current word\n",
        "            curr_id_probs.append(probs.numpy()[0][curr_id])\n",
        "\n",
        "            # append current token ID to list of previous words (if there are any)\n",
        "            # reason: The previous parts of the word are part of the context.\n",
        "            ids_list = ids_list + [curr_id]\n",
        "            ids_tensor = torch.tensor(ids_list).unsqueeze(0) # put the token IDs into a tensor\n",
        "\n",
        "            # -------------------\n",
        "            \"\"\" How do I deal with target words that consist of several tokens? \"\"\"\n",
        "            # (This is more an explanation for future-me, because I will definitely forget\n",
        "            # I even thought about how to deal with this question.)\n",
        "\n",
        "            # If the to-be-predicted word consists of several tokens,\n",
        "            # I have to predict each of them and give the previous\n",
        "            # tokens of the word to the input chunk.\n",
        "\n",
        "            # Example (I made up the tokens btw): El-ef-ant\n",
        "            # In this case, I generate a surprisal score for \"El\" given\n",
        "            # the input chunk with n masked words. Then I generate surprisal\n",
        "            # scores for the next token \"ef\" given the  input chunk with n masked\n",
        "            # words and the previous token of the current word (\"El\"), but\n",
        "            # I won't mask \"El\" here.\n",
        "\n",
        "            # Reason:\n",
        "            # I want 1 surprisal score for 1 word, so even if this isn't quite true,\n",
        "            # 1 word means 1 prediction for me (at least in the brain, I don't think it tokenizes words\n",
        "            # like GPT-2 does, at least not in the exact same way).\n",
        "            # So I think you should get all previous tokens from the current word without masking them.\n",
        "            # Another thing is that if I masked the start of the word itself, too,\n",
        "            # for some words, my suprisal scores would go completely through the roof,\n",
        "            # just because they are longer than others.\n",
        "\n",
        "            # -------------------\n",
        "\n",
        "            # Long story short: Add 1 token to the end of the attention\n",
        "            # mask as well, but don't mask the new one (so the attention mask has a value of 1 there).\n",
        "            additional_token_mask = torch.ones((1, 1), dtype = torch.long)\n",
        "            curr_attention_mask = torch.cat([curr_attention_mask, additional_token_mask], dim = 1)\n",
        "            print(\"attention mask:\", curr_attention_mask)\n",
        "\n",
        "        # multiply all probabilities for current word:\n",
        "        act_word_prob = np.prod(curr_id_probs)\n",
        "\n",
        "        # transform probability value into surprisal score (negative log of the probability)\n",
        "        # negative log = log(1 / x) with x being the value you want to get the negative log of.\n",
        "        # I use e as a base value for the log here.\n",
        "        surprisal_score = np.log( 1 / act_word_prob )\n",
        "\n",
        "        # if surprisal score == Inf, set surprisal score to 100\n",
        "        if surprisal_score == float('inf'):\n",
        "          surprisal_score = 100\n",
        "\n",
        "        # collect surprisal score in array for all surprisal scores of current time scale\n",
        "        curr_TS_list.append(surprisal_score)\n",
        "        print(\"\\nSurprisal score for actual word \" + actual_word +\" is \" + str(surprisal_score) + \".\")\n",
        "        print(\"TS = \" + str(curr_ts) + \"- Text Nr = \" + str(text_nr) + \" - Trial Nr = \" + str(word_idx))\n",
        "        print(\" --------------- \")\n",
        "\n",
        "print(\"\\n\\n\\nfinished computing surprisal scores\\n\\n\")\n",
        "\n",
        "\n",
        "# append new surprisal score columns to texts_df\n",
        "texts_df = texts_df.assign(surprisal_1  = surprisal_1,\n",
        "                           surprisal_4  = surprisal_4,\n",
        "                           surprisal_12  = surprisal_12,\n",
        "                           surprisal_60  = surprisal_60)\n",
        "\n",
        "\n",
        "\"\"\" download texts_df as surprisal_scores_masked_context.csv \"\"\"\n",
        "texts_df.to_csv(\"surprisal_scores_masked_context.csv\", encoding = \"utf-8-sig\")\n",
        "files.download(\"surprisal_scores_masked_context.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "hMKI2Fn_JElI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In case the chunk before throws an error because you used only a subset of the texts,\n",
        "# you can run this to save the data:\n",
        "\n",
        "#texts_df = texts_df[texts_df['text_nr'] == 'text_10']\n",
        "\n",
        "#texts_df = texts_df.assign(surprisal_1  = surprisal_1,\n",
        "#                           surprisal_4  = surprisal_4,\n",
        "#                           surprisal_12  = surprisal_12,\n",
        "#                           surprisal_60  = surprisal_60)\n",
        "\n",
        "#texts_df.to_csv(\"surprisal_scores_masked_context.csv\", encoding = \"utf-8-sig\")\n",
        "#files.download(\"surprisal_scores_masked_context.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "r-OGAidyyx90",
        "outputId": "65b9187d-b230-4ddc-f9af-2a487101a61c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_65fc1e58-a1a0-4c64-b311-9df3129a471c\", \"surprisal_scores_masked_context_text10.csv\", 29898)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Test cases to check whether the attention mask is set on\n",
        "    all layers and the predictions are generated as intended.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" TEST 1 \"\"\"\n",
        "# Get attention weights from all layers and see whether\n",
        "# they are set to 0 on all layers for the tokens I want to mask.\n",
        "\n",
        "def print_layer_attention_weights(previous_words, n, model):\n",
        "\n",
        "    # generate token ids for each of the x previous words\n",
        "    encoded_input = tokenizer(\" \".join(previous_words),\n",
        "                              add_special_tokens = False,\n",
        "                              return_tensors = \"pt\")\n",
        "\n",
        "    # get token IDs, put them in a list and in a tensor\n",
        "    ids_tensor = encoded_input['input_ids']\n",
        "    ids_list = ids_tensor[0].tolist()\n",
        "    print(\"input IDs:\", ids_list)\n",
        "\n",
        "    # get original attention mask\n",
        "    curr_attention_mask = encoded_input.attention_mask.clone()\n",
        "\n",
        "    # mask the last n words\n",
        "    unmasked_words = previous_words[:-n]\n",
        "    unmasked_tokens = tokenizer.tokenize(\" \".join(unmasked_words))\n",
        "\n",
        "    # get the last n words and the number of tokens to mask\n",
        "    masked_words = previous_words[-n:]\n",
        "    masked_tokens = tokenizer.tokenize(\" \".join(masked_words))\n",
        "    n_tokens = len(masked_tokens)\n",
        "    print(\"nr of masked tokens:\", n_tokens)\n",
        "\n",
        "    # modify the original attention mask: set weights to 0 where the input should be masked\n",
        "    curr_attention_mask[:, -n_tokens:] = 0\n",
        "    print(\"attention mask:\", curr_attention_mask)\n",
        "\n",
        "    # Generate model output\n",
        "    output = model.generate(ids_tensor,\n",
        "                            attention_mask = curr_attention_mask,\n",
        "                            return_dict_in_generate = True, # for some reason I need to include this or I don't get attention weights in the output.\n",
        "                            output_attentions = True,\n",
        "                            max_new_tokens = 1)\n",
        "\n",
        "    generated_tokens = output.sequences\n",
        "    attentions = output.attentions\n",
        "\n",
        "    #print(\"\\ninput tokens + predicted token(s):\", generated_tokens)\n",
        "    #print(\"\\n\\nattention weights:\\n\", attentions)\n",
        "\n",
        "    # average the attention weights over the generated tokens for each layer:\n",
        "    layer_attention_weights = []\n",
        "    for i, layer_attentions in enumerate(attentions[0]):\n",
        "      #print(i, \"\\n\\n\\ncurrent attention weights for all heads:\\n\\n\", layer_attentions)\n",
        "      layer_attention_weights.append(torch.mean(layer_attentions[:, -1, :-n_tokens], dim=1))\n",
        "\n",
        "    # print layer attention weights\n",
        "    for i, layer_weights in enumerate(layer_attention_weights):\n",
        "        print(f\"\\n\\nLayer {i+1} attention weights:\\n\")\n",
        "        print(layer_weights.tolist())\n",
        "\n",
        "# Test with example input\n",
        "#input_text = [\"Orlando\", \"liebte\", \"von\", \"Natur\", \"aus\", \"einsame\", \"Orte,\", \"weite\", \"Ausblicke\", \"und\", \"das\", \"Gefühl,\", \"für\", \"immer\", \"und\", \"ewig\"] # allein zu sein.\n",
        "input_text = [\"Orlando\", \"liebte\", \"von\", \"Natur\"]\n",
        "n = 1  # I want to mask the last n words\n",
        "print_layer_attention_weights(input_text, n, model)\n",
        "\n",
        "# Works. The last n values in each attention weights tensor are always 0, just as intended.\n"
      ],
      "metadata": {
        "id": "PAkDM-M2uoOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" TEST 2 \"\"\"\n",
        "# Here I'm turning the masking around a bit - I will mask the first instead of the last n words.\n",
        "# I will then compare the predictions for the partially masked input with a \"normal\" prediction\n",
        "# with an input chunk where the words that were masked in the first run are just left out.\n",
        "\n",
        "# I'm not quite sure if this really makes sense, but at least for a human it should be the\n",
        "# same because syntactically, it shouldn't matter that there are a few masked words at the\n",
        "# beginning of the context chunk.\n",
        "\n",
        "# In test 3 I'll do it with the first n words, where it should definitely matter (at least for the syntax)\n",
        "# if we include & mask them or exclude them completely.\n",
        "\n",
        "def exclude_first_n(previous_words, n, model):\n",
        "\n",
        "    seed = 42\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    \"\"\" 1. mask first n words & run prediction \"\"\"\n",
        "    # generate token ids for each of the x previous words\n",
        "    encoded_input = tokenizer(\" \".join(previous_words),\n",
        "                              add_special_tokens = False,\n",
        "                              return_tensors = \"pt\")\n",
        "\n",
        "    # get token IDs, put them in a list and in a tensor\n",
        "    ids_tensor = encoded_input['input_ids']\n",
        "    ids_list = ids_tensor[0].tolist()\n",
        "    #print(\"input IDs:\", ids_list)\n",
        "\n",
        "    # get original attention mask\n",
        "    curr_attention_mask = encoded_input.attention_mask.clone()\n",
        "\n",
        "    # mask the first n words\n",
        "    unmasked_words = previous_words[n:]\n",
        "    unmasked_tokens = tokenizer.tokenize(\" \".join(unmasked_words))\n",
        "    #print(\"unmasked_words\", unmasked_words)\n",
        "\n",
        "    # get the first n words and the number of tokens to mask\n",
        "    masked_words = previous_words[:n]\n",
        "    #print(\"masked words:\", masked_words)\n",
        "    masked_tokens = tokenizer.tokenize(\" \".join(masked_words))\n",
        "    n_tokens = len(masked_tokens)\n",
        "    #print(\"nr of masked tokens:\", n_tokens)\n",
        "\n",
        "    # modify the original attention mask: set weights to 0 where the input should be masked\n",
        "    curr_attention_mask[:, :n_tokens] = 0\n",
        "    #print(\"attention mask:\", curr_attention_mask)\n",
        "\n",
        "    # Generate model output\n",
        "    output = model.generate(ids_tensor,\n",
        "                            attention_mask = curr_attention_mask,\n",
        "                            return_dict_in_generate = True,\n",
        "                            output_scores = True,\n",
        "                            max_new_tokens = 13)\n",
        "\n",
        "    predicted_text = tokenizer.decode(output.sequences.numpy().tolist()[0],\n",
        "                                      skip_special_tokens = True)\n",
        "\n",
        "    print(\"Prediction with the first n words masked:\", predicted_text)\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\" 2. exclude first n words & run prediction \"\"\"\n",
        "    # do the same again, but exclude the first n words instead of masking them\n",
        "\n",
        "    # exclude first n words from the input chunk\n",
        "    included_words = previous_words[n:]\n",
        "    #print(\"\\n\\nincluded words\", included_words)\n",
        "\n",
        "    # encode text, generate token IDs tensor\n",
        "    encoded_input = tokenizer(\" \".join(included_words),\n",
        "                              add_special_tokens = False,\n",
        "                              return_tensors=\"pt\")\n",
        "\n",
        "    # get token IDs, put them in a list and in a tensor\n",
        "    ids_tensor = encoded_input['input_ids']\n",
        "    ids_list = ids_tensor[0].tolist()\n",
        "    #print(\"input IDs:\", ids_list)\n",
        "\n",
        "    # generate model output\n",
        "    output = model.generate(ids_tensor,\n",
        "                            return_dict_in_generate = True,\n",
        "                            output_scores = True,\n",
        "                            max_new_tokens = 13)\n",
        "    # decode IDs to get predicted text\n",
        "    predicted_text = tokenizer.decode(output.sequences.numpy().tolist()[0],\n",
        "                                      skip_special_tokens = True)\n",
        "\n",
        "    print(\"Prediction with the first n words excluded:\", predicted_text)\n",
        "\n",
        "\n",
        "# Test with example input\n",
        "input_text = [\"Orlando\", \"liebte\", \"von\", \"Natur\", \"aus\", \"einsame\", \"Orte,\", \"weite\", \"Ausblicke\", \"und\", \"das\", \"Gefühl,\", \"für\", \"immer\", \"und\", \"ewig\", \"allein\", \"zu\", \"sein.\"]\n",
        "#input_text = [\"Orlando\", \"liebte\", \"von\", \"Natur\"]\n",
        "n = 10  # I want to mask/exclude the first n words\n",
        "exclude_first_n(input_text, n, model)\n",
        "\n",
        "# Seems to be the same (the first part of the sentence is not the same obviously, but that\n",
        "# was the masked/excluded part of the input. The predictions are the same.)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWM-ZWcuP6Zb",
        "outputId": "264d3bde-4901-408f-e2e7-f11cdcb30cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction with the first n words masked: Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig allein zu sein.\n",
            "Ich bin nicht allein.\n",
            "Ich bin ein Teil von dir\n",
            "Prediction with the first n words excluded: das Gefühl, für immer und ewig allein zu sein.\n",
            "Ich bin nicht allein.\n",
            "Ich bin ein Teil von dir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" TEST 3 \"\"\"\n",
        "# Here I'm doing the same as in test 2, but I'm masking the last n words, just as I did\n",
        "# for the surprisal score calculation.\n",
        "# I will then compare the predictions for the partially masked input with a \"normal\" prediction\n",
        "# with an input chunk where the words that were masked in the first run are just left out.\n",
        "\n",
        "# Unlike in test 2, where I wasn't so sure whether the masking/exclusion of words\n",
        "# should make a difference, here it should definitely matter (at least for the syntax)\n",
        "# if we include & mask them or exclude them completely.\n",
        "\n",
        "def exclude_last_n(previous_words, n, model):\n",
        "\n",
        "    # Idk if this actually matters for the generation (probably not),\n",
        "    # but set seed:\n",
        "    seed = 42\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    \"\"\" 1. mask last n words & run prediction \"\"\"\n",
        "    # generate token ids for each of the x previous words\n",
        "    encoded_input = tokenizer(\" \".join(previous_words),\n",
        "                              add_special_tokens = False,\n",
        "                              return_tensors = \"pt\")\n",
        "\n",
        "    # get token IDs, put them in a list and in a tensor\n",
        "    ids_tensor = encoded_input['input_ids']\n",
        "    ids_list = ids_tensor[0].tolist()\n",
        "    #print(\"input IDs:\", ids_list)\n",
        "\n",
        "    # get original attention mask\n",
        "    curr_attention_mask = encoded_input.attention_mask.clone()\n",
        "\n",
        "    # check which TS index we have here: If it's the\n",
        "    # first TS, we don't have to mask anything.\n",
        "    unmasked_words = previous_words[:-n]\n",
        "    unmasked_tokens = tokenizer.tokenize(\" \".join(unmasked_words))\n",
        "    #print(\"unmasked tokens:\", unmasked_tokens)\n",
        "\n",
        "    # get the last n words and the number of tokens to mask\n",
        "    masked_words = previous_words[-n:]\n",
        "    masked_tokens = tokenizer.tokenize(\" \".join(masked_words))\n",
        "    n_tokens = len(masked_tokens)\n",
        "    #print(\"number of tokens to be masked:\", n_tokens)\n",
        "    print(\"masking the following\", n, \"words now:\", masked_words, \" - list of masked tokens:\", masked_tokens)\n",
        "\n",
        "    # modify the original attention mask: set weights to 0 where the input should be masked\n",
        "    # --> I think if we set this attention mask in the .generate() call, we set this for all layers.\n",
        "    #print(\"original attention mask:\", curr_attention_mask)\n",
        "    curr_attention_mask[:, -n_tokens:] = 0\n",
        "    #print(\"attention mask: \", curr_attention_mask)\n",
        "\n",
        "    # Generate model output\n",
        "    output = model.generate(ids_tensor,\n",
        "                            attention_mask = curr_attention_mask,\n",
        "                            return_dict_in_generate = True,\n",
        "                            output_scores = True,\n",
        "                            max_new_tokens = 13)\n",
        "\n",
        "    predicted_text = tokenizer.decode(output.sequences.numpy().tolist()[0],\n",
        "                                      skip_special_tokens = True)\n",
        "\n",
        "    print(\"Prediction with the last n words masked:\", predicted_text)\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\" 2. exclude last n words & run prediction \"\"\"\n",
        "    # do the same again, but exclude the first n words instead of masking them\n",
        "\n",
        "    # exclude first n words from the input chunk\n",
        "    included_words = previous_words[:-n]\n",
        "    print(\"\\n\\nincluded words\", included_words)\n",
        "\n",
        "    # encode text, generate token IDs tensor\n",
        "    encoded_input = tokenizer(\" \".join(included_words),\n",
        "                              add_special_tokens = False,\n",
        "                              return_tensors=\"pt\")\n",
        "\n",
        "    # generate model output\n",
        "    output = model.generate(ids_tensor,\n",
        "                            return_dict_in_generate = True,\n",
        "                            output_scores = True,\n",
        "                            max_new_tokens = 13)\n",
        "    # decode IDs to get predicted text\n",
        "    predicted_text = tokenizer.decode(output.sequences.numpy().tolist()[0],\n",
        "                                      skip_special_tokens = True)\n",
        "\n",
        "    print(\"Prediction with the last n words excluded:\", predicted_text)\n",
        "\n",
        "\n",
        "# Test with example input\n",
        "input_text = [\"Orlando\", \"liebte\", \"von\", \"Natur\", \"aus\", \"einsame\", \"Orte,\", \"weite\", \"Ausblicke\", \"und\", \"das\", \"Gefühl,\", \"für\", \"immer\", \"und\", \"ewig\", \"allein\", \"zu\", \"sein.\"]\n",
        "n = 6  # I want to mask/exclude the first n words\n",
        "exclude_last_n(input_text, n, model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf31B4XGYQBX",
        "outputId": "93200305-338d-47b0-8e7a-229495fe839f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masking the following 6 words now: ['immer', 'und', 'ewig', 'allein', 'zu', 'sein.']  - list of masked tokens: ['immer', 'Ġund', 'Ġewig', 'Ġallein', 'Ġzu', 'Ġsein', '.']\n",
            "Prediction with the last n words masked: Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig allein zu sein. wenn es nicht zu Hause war.\n",
            "Er war ein guter Freund\n",
            "\n",
            "\n",
            "included words ['Orlando', 'liebte', 'von', 'Natur', 'aus', 'einsame', 'Orte,', 'weite', 'Ausblicke', 'und', 'das', 'Gefühl,', 'für']\n",
            "Prediction with the last n words excluded: Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig allein zu sein.\n",
            "Er war ein großer Künstler, ein Künstler, der sich in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How to compute cosine similarity for 2 texts\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Example sentences\n",
        "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence2 = \"Sphinx of black quartz, judge my vow.\"\n",
        "sentence3 = \"The quick brown fox jumps over the sphinx of black quartz.\"\n",
        "\n",
        "# Compute cosine similarity for sentences 1 & 2\n",
        "# 0 = completely different texts\n",
        "# 1 = very similar (e.g. same words in different order) or identical\n",
        "vectorizer1 = CountVectorizer().fit_transform([sentence1, sentence2])\n",
        "cos_sim = cosine_similarity(vectorizer1)[0][1]\n",
        "print(\"Cosine similarity for sentences 1 & 2:\", cos_sim)\n",
        "\n",
        "# compare sentences 1 & 3:\n",
        "vectorizer2 = CountVectorizer().fit_transform([sentence1, sentence3])\n",
        "cos_sim = cosine_similarity(vectorizer2)[0][1]\n",
        "print(\"Cosine similarity for sentences 1 & 3:\", cos_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxVrXy1a_tdf",
        "outputId": "cb7252ad-47cb-4817-a008-dece7d473f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity for sentences 1 & 2: 0.0\n",
            "Cosine similarity for sentences 1 & 3: 0.7526178090063818\n"
          ]
        }
      ]
    }
  ]
}