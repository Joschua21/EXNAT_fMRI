{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "import modules"
      ],
      "metadata": {
        "id": "3pTBHQnoEZcZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUTDJiTZCBX8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "import torch"
      ],
      "metadata": {
        "id": "mRMHe5VwES3x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load model"
      ],
      "metadata": {
        "id": "3w6a1LrUEcLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/german-gpt2\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"dbmdz/german-gpt2\", pad_token_id=tokenizer.eos_token_id) # check EOS"
      ],
      "metadata": {
        "id": "I2VrgZlyEUyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "getting to know the tokenizer"
      ],
      "metadata": {
        "id": "jx7YOytXJ-8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = 'Gartenhaus'\n",
        "txt2id = tokenizer.encode(txt)\n",
        "print(\"The word '\" + txt + \"' is represented by token ID(s) \" + str(txt2id) + \".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2xCcgyGVlz9",
        "outputId": "2ff7a17f-95f0-4677-b3ff-4d0232dd1beb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word 'Gartenhaus' is represented by token ID(s) [27361, 951].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id = 10737\n",
        "id2txt = tokenizer.decode(id)\n",
        "print(\"Token ID \" + str(id) + \" represents the word '\" + id2txt[1::] + \"'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esfN8wqRKB2P",
        "outputId": "29047398-8b6b-4b02-ef58-9ff24d887e37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token ID 10737 represents the word 'Erinnerung'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate and read out predictions"
      ],
      "metadata": {
        "id": "3kYUppwyIrkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_list = tokenizer.encode(\"An einem kalten Tag im Februar\")\n",
        "ids_array = np.expand_dims((ids_list), axis=0)\n",
        "output = model.generate(torch.tensor(ids_array), return_dict_in_generate=True, output_scores=True, max_new_tokens=20) # set output length here!"
      ],
      "metadata": {
        "id": "ekN7JhB-XjpM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word prediction using greedy search: just to showcase the capabilities of the model ;-)\n",
        "predicted_ids = output.sequences\n",
        "predicted_txt = tokenizer.decode(predicted_ids.numpy().tolist()[0])\n",
        "\n",
        "print(\"Predicted text: \" + predicted_txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkhbFHtcHR2h",
        "outputId": "57ecae41-0fa4-4a1d-d8ce-01c3a5bc60a4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text: An einem kalten Tag im Februar, als die Sonne noch nicht untergegangen war, war es in der Stadt so hei√ü, dass die\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read out next-word probabilities for all words in the vocabulary\n",
        "logits = output.scores[0]\n",
        "probs = tf.nn.softmax(logits)"
      ],
      "metadata": {
        "id": "WZ9kev3LeutU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check which word has the highest probability of being the next word\n",
        "\n",
        "print(\"highest probability score: \" + str(np.max(probs)))\n",
        "\n",
        "max_id = np.argmax(probs)\n",
        "print(\"token corresponding to the highest probability score: \" + str(tokenizer.decode(max_id))) # well, in this case it's a comma, sorry for the non-ideal example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HP3ovj7Tn79",
        "outputId": "ddb466a8-5366-4d9e-fdde-f5998b4920d2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "highest probability score: 0.07783756\n",
            "token corresponding to the highest probability score: ,\n"
          ]
        }
      ]
    }
  ]
}