{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "import modules"
      ],
      "metadata": {
        "id": "3pTBHQnoEZcZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUTDJiTZCBX8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "import torch"
      ],
      "metadata": {
        "id": "mRMHe5VwES3x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model\n",
        "We want a GPT-2 model that's trained on German texts: https://huggingface.co/dbmdz/german-gpt2\n",
        "\n"
      ],
      "metadata": {
        "id": "3w6a1LrUEcLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/german-gpt2\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"dbmdz/german-gpt2\", pad_token_id=tokenizer.eos_token_id) # check EOS"
      ],
      "metadata": {
        "id": "I2VrgZlyEUyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import CSV with texts used in EXNAT-1 \n",
        "The csv contains 4 columns: \n",
        "\n",
        "word_punct = single words with punctuation\n",
        "\n",
        "word_no_punct = single words without punctuation (exception: dashes between 2 words)\n",
        "\n",
        "text_nr = Number of text, text_01 - text_09\n",
        "\n",
        "trial_nr = Index of word in the text, starting with 1. There are 300 words (aka trials) in each text.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*Hint: Make sure you upload the csv in Google Colab before trying to read it in. You can upload the csv by clicking the file icon in the side bar.*"
      ],
      "metadata": {
        "id": "9gSRAlvGQFUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in CSV with texts\n",
        "texts_df = pd.read_csv('/content/Texts_surprisal_scores.csv', sep=\";\")"
      ],
      "metadata": {
        "id": "ZAnULYTnQRkg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Compute Surprisal Scores for each Word\n",
        "\n",
        "Loop texts in Text_surprisal_scres.csv. For each word, get chunk with x previous words and calculate probability for predicting the actual word in the text. "
      ],
      "metadata": {
        "id": "gvR6uoMkSvt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Compute surprisal scores with different \"context\" chunk sizes \"\"\"\n",
        "# (running this takes about 32 minutes)\n",
        "\n",
        "# get unique text numbers in texts_df\n",
        "text_nrs = list(set(texts_df[\"text_nr\"]))\n",
        "#print(\"preparing surprisal scores for the following texts:\")\n",
        "#print(text_nrs)\n",
        "\n",
        "# collect surprisal scores for different chunk sizes here.\n",
        "# The chunk sizes I use here are completely arbitrary values.\n",
        "word_surprisal_128 = []\n",
        "word_surprisal_100 = []\n",
        "word_surprisal_64  = []\n",
        "word_surprisal_50  = []\n",
        "word_surprisal_32  = []\n",
        "word_surprisal_16  = []\n",
        "word_surprisal_8   = []\n",
        "word_surprisal_4   = []\n",
        "word_surprisal_3   = []\n",
        "word_surprisal_2   = []\n",
        "word_surprisal_1   = []\n",
        "\n",
        "\n",
        "chunk_size = [128, 100, 64, 50, 32, 16, 8, 4, 3, 2, 1]\n",
        "chunk_size_lists = [word_surprisal_128, word_surprisal_100, word_surprisal_64, \n",
        "                    word_surprisal_50, word_surprisal_32, word_surprisal_16,\n",
        "                    word_surprisal_8, word_surprisal_4, word_surprisal_3, \n",
        "                    word_surprisal_2, word_surprisal_1]\n",
        "\n",
        "# loop chunk sizes so I get 11 lists with surprisal scores\n",
        "for chunk_idx in range(0, len(chunk_size)):\n",
        "    print(\"start preparing surprisal scores for chunk size = \" + str(chunk_size[chunk_idx]))\n",
        "\n",
        "    \"\"\" loop texts \"\"\"\n",
        "    for text_nr in text_nrs:\n",
        "      # get subset of df with current text\n",
        "      curr_text = texts_df[texts_df[\"text_nr\"] == text_nr]\n",
        "\n",
        "      \"\"\" loop words - for chunk size = x, start at idx = x (aka with the x + 1th word) \"\"\"\n",
        "      chunk_size_lists[chunk_idx] = chunk_size_lists[chunk_idx] + [None] * chunk_size[chunk_idx] # for chunk size x, the first x words don't get surprisal scores\n",
        "      \n",
        "      for word_idx in range(chunk_size[chunk_idx], len(curr_text[\"word_punct\"])):\n",
        "\n",
        "        # get x previous words as text string (careful, get words + punctuation!)\n",
        "        previous_words = list(curr_text[\"word_punct\"])[word_idx - chunk_size[chunk_idx] : word_idx]\n",
        "        # turn list of previous words into 1 string\n",
        "        previous_words = ' '.join(previous_words)\n",
        "\n",
        "        \"\"\" Predict next word \"\"\"\n",
        "        # get current word (with punctuation):\n",
        "        actual_word = list(curr_text[\"word_punct\"])[word_idx]\n",
        "\n",
        "        \"\"\" Generate prediction for next token(s) \"\"\"\n",
        "        ids_list = tokenizer.encode(previous_words) # generate token ids for each of the x previous words\n",
        "        ids_array = np.expand_dims((ids_list), axis = 0) # put the token IDs into array\n",
        "        # predict the next x words\n",
        "        output = model.generate(torch.tensor(ids_array), \n",
        "                                return_dict_in_generate = True, \n",
        "                                output_scores = True, \n",
        "                                max_new_tokens = 1) # set output length here!\n",
        "        \n",
        "        \"\"\" read out next-word probabilities for all words in the vocabulary \"\"\"\n",
        "        logits = output.scores[0] # logits = probabilities with range [0,1] transformed to range [inf, -inf]\n",
        "        probs = tf.nn.softmax(logits) # transform logits back to probabilities\n",
        "\n",
        "        \"\"\" Get probability for actual next word \"\"\"\n",
        "        # We should also predict punctuation. \n",
        "        # It's not like the words are shown without punctuation on screen.\n",
        "\n",
        "        # get probability for actual word\n",
        "        # problem: actual word might have multiple token IDs\n",
        "        act_word_id = tokenizer.encode(actual_word) #Output looks somewhat like this: [44, 305, 479, 5283]\n",
        "\n",
        "        # get probabilities for token ids of the current word\n",
        "        act_word_probs = []\n",
        "        for curr_token in act_word_id:\n",
        "          #print(tokenizer.decode(curr_token)) # print single tokens (e.g. tokens \"Gold\" & \"Grube\" for word \"Goldgrube\")\n",
        "          # get probability for current token id and append to list of probabilities.\n",
        "          act_word_probs.append(probs.numpy()[0][curr_token]) \n",
        "\n",
        "        # multiply values in the list\n",
        "        act_word_prob = np.prod(act_word_probs)\n",
        "\n",
        "\n",
        "        # transform probability value into surprisal score (negative log of the probability)\n",
        "        # neg log = log(1 / x) with x being the value you want to get the neg log of.\n",
        "        # I use e as a base value for the log here.\n",
        "        surprisal_score = np.log( 1 / act_word_prob )\n",
        "\n",
        "        print(\"Chunk size = \" + str(chunk_size[chunk_idx]) + \" - Surprisal score for actual word \" + actual_word +\" is \" + str(surprisal_score) + \".\")\n",
        "        print(\"Text Nr = \" + str(text_nr) + \" - Trial Nr = \" + str(word_idx))\n",
        "\n",
        "        # append to array where all surprisal scores are collected\n",
        "        chunk_size_lists[chunk_idx].append(surprisal_score)\n",
        "\n",
        "print(\"finished generating surprisal scores\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" append new surprisal score columns to texts_df \"\"\"\n",
        "\n",
        "surprisal_128 = chunk_size_lists[0]\n",
        "surprisal_100 = chunk_size_lists[1]\n",
        "surprisal_64  = chunk_size_lists[2]\n",
        "surprisal_50  = chunk_size_lists[3]\n",
        "surprisal_32  = chunk_size_lists[4]\n",
        "surprisal_16  = chunk_size_lists[5]\n",
        "surprisal_8   = chunk_size_lists[6]\n",
        "surprisal_4   = chunk_size_lists[7]\n",
        "surprisal_3   = chunk_size_lists[8]\n",
        "surprisal_2   = chunk_size_lists[9]\n",
        "surprisal_1   = chunk_size_lists[10]\n",
        "\n",
        "\n",
        "texts_df = texts_df.assign(surprisal_1 = surprisal_1, \n",
        "                           surprisal_2 = surprisal_2,\n",
        "                           surprisal_3 = surprisal_3,\n",
        "                           surprisal_4 = surprisal_4,\n",
        "                           surprisal_8 = surprisal_8,\n",
        "                           surprisal_16 = surprisal_16,\n",
        "                           surprisal_32 = surprisal_32,\n",
        "                           surprisal_50 = surprisal_50,\n",
        "                           surprisal_64 = surprisal_64,\n",
        "                           surprisal_100 = surprisal_100,\n",
        "                           surprisal_128 = surprisal_128)\n",
        "\n",
        "# print first 370 rows of df to check if it looks correct\n",
        "# (depending on chunk size x, first x values of a text should be NaN)\n",
        "#print(texts_df.head(370))\n",
        "\n",
        "\"\"\" download texts_df as surprisal_scores.csv \"\"\"\n",
        "texts_df.to_csv('surprisal_scores.csv', encoding = 'utf-8-sig') \n",
        "#surprisal_scores.csv.download('surprisal_scores.csv')\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"surprisal_scores.csv\")\n"
      ],
      "metadata": {
        "id": "oghDmZhgTcVs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff2a3ef-6eae-40cb-86af-656d6838a3d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start preparing surprisal scores for chunk size = 128\n",
            "Chunk size = 128 - Surprisal score for actual word Petra is 35.31081098538522.\n",
            "Text Nr = text_08 - Trial Nr = 128\n",
            "Chunk size = 128 - Surprisal score for actual word kreuzten is 30.21974867532466.\n",
            "Text Nr = text_08 - Trial Nr = 129\n",
            "Chunk size = 128 - Surprisal score for actual word sich is 13.867515650127752.\n",
            "Text Nr = text_08 - Trial Nr = 130\n",
            "Chunk size = 128 - Surprisal score for actual word mehrere is 32.59679126579894.\n",
            "Text Nr = text_08 - Trial Nr = 131\n",
            "Chunk size = 128 - Surprisal score for actual word Handelswege, is 33.480889525337474.\n",
            "Text Nr = text_08 - Trial Nr = 132\n",
            "Chunk size = 128 - Surprisal score for actual word darunter is 14.453735272480584.\n",
            "Text Nr = text_08 - Trial Nr = 133\n",
            "Chunk size = 128 - Surprisal score for actual word die is 11.889444721091245.\n",
            "Text Nr = text_08 - Trial Nr = 134\n",
            "Chunk size = 128 - Surprisal score for actual word uralte is 34.30237511398857.\n",
            "Text Nr = text_08 - Trial Nr = 135\n",
            "Chunk size = 128 - Surprisal score for actual word Weihrauchstraße. is 69.26084982109917.\n",
            "Text Nr = text_08 - Trial Nr = 136\n",
            "Chunk size = 128 - Surprisal score for actual word Binnen is 16.936305857063573.\n",
            "Text Nr = text_08 - Trial Nr = 137\n",
            "Chunk size = 128 - Surprisal score for actual word weniger is 10.485545246535471.\n",
            "Text Nr = text_08 - Trial Nr = 138\n",
            "Chunk size = 128 - Surprisal score for actual word Jahrzehnte is 53.07368034450767.\n",
            "Text Nr = text_08 - Trial Nr = 139\n",
            "Chunk size = 128 - Surprisal score for actual word entstanden is 33.056545175004686.\n",
            "Text Nr = text_08 - Trial Nr = 140\n",
            "Chunk size = 128 - Surprisal score for actual word hunderte is 38.41511281230783.\n",
            "Text Nr = text_08 - Trial Nr = 141\n",
            "Chunk size = 128 - Surprisal score for actual word Höhlen is 32.79813989745229.\n",
            "Text Nr = text_08 - Trial Nr = 142\n",
            "Chunk size = 128 - Surprisal score for actual word mit is 13.63144302946072.\n",
            "Text Nr = text_08 - Trial Nr = 143\n"
          ]
        }
      ]
    }
  ]
}