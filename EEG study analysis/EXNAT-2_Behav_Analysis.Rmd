---
title: "EXNAT 2 – Analysis of Behavioural Data"
author: "Merle Schuckart"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r file setup, echo = FALSE}
rm(list = ls()) # clear environment
knitr::opts_chunk$set() # set default options for all code blocks in this document
options(scipen = 999) # don't use scientific notation for very large or small numbers

```




## Load packages
```{r packages, echo = FALSE, message = FALSE, warning = TRUE}

# Create a list with needed libraries
pkgs <- c("here", # for working with relative paths
          "ggeffects", # create data frames of marginal effects for 'ggplot' from model outputs
          "rstudioapi", # for getting path of this file
          "stringr", # for getting substrings
          "ggplot2", # for plots
          "dplyr", # for replacing multiple values in vector with different values
          "reshape2", # for reshaping df format
          "data.table", # for data tables
          "tidyverse", # for aggregating
          "R.utils", # for deleting all empty csv files in the directory
          "psycho", # for computing d-prime values
          "yarrr", # for plotting
          "devtools", # for getting packages from github
          "lattice", # for quick & dirty density plots
          "gtools", # for getting tuples from list
          "see", # for plotting residuals of lme4 model
          "performance", # needed for plotting the lme4 model output summary
          "lme4", # also for linear mixed models, but no p-values in the summary
          "car", # for basic Anova() function & aGSIFs (= basically weird adjusted VIFs)
          "sjPlot", # for running Anova & showing results in a HTML table in the Viewer
          "BayesFactor", # that's self-explanatory
          "interactions", # for simple slopes analysis
          "patchwork", # needed for check_model plotting function
          "glmnet", # for elastic net regression
          "caret", # for choosing the best tuning parameters for elastic net regression
          "beepr", # to play Super Mario sound when script finished running
          "effects", # for plotting lmm interaction & main effects
          "interactions", # for simple slope analysis plots
          "gridExtra") # for showing several plots in a grid

# Load each listed library, check if it's already installed
# and install if necessary
for (pkg in pkgs){
  if(!require(pkg, character.only = TRUE)){
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}

# install package with lists of stop words from github
devtools::install_github("quanteda/stopwords")
library(stopwords)


# set path to the current file so we have the EEG study analysis folder as our working directory
here::i_am("EXNAT-2_Behav_Analysis.Rmd") 

```


## Load d-prime function from different R script:
```{r source d-prime function, echo = FALSE}
source(here::here("get_dprime.R")) # get script for computing d-primes

```


## Settings for plots
```{r Theme for plots, echo = FALSE}

# set custom colour palettes you want to use:

# for distinguishing between cognitive load conditions
# (green = Baseline, yellow = 1-back, red = 2-back)
palet1_light <- c("darkseagreen", "darkgoldenrod1", "indianred")
palet1_dark  <- c("darkseagreen4", "darkgoldenrod3", "indianred4")

palet2 <- c("#575A7B", "#ED9201", "#982126") 
# --> used these on my poster for the different n-back conditions, should be distinguishable even for people with impaired colour vision

# for distinguishing between age groups
# light blue: younger (<= 60), dark blue: older (> 60)
palet3_light <- c("#7EB0D1", "#38759C") 
palet3_dark <- c("#38759C", "#266187") 


```


## Set path to datasets
```{r path setup, echo = FALSE}
# each participant has 1 folder, and all of those folders are in the "Data" folder:
path_data_folder <- here::here("Data/")
#print(path_data_folder)
```


```{r read in and preprocess data, echo = FALSE}

# Get list of all participant folders from the data folder:
file_list <- list.files(path = path_data_folder, pattern='part')
# To get all .csv files, we need to loop over these folders

# placeholders df for demographics, questions & text data
df_demogr_all           <- data.frame()
df_text_data        <- data.frame()
df_comprehension_Qs <- data.frame()


# loop folders
for (i in 1:length(file_list)) {

  # READ IN DATA
  
  # find the csv file that contains our PsychoPy experimental data
  # --> should be the file that has EXNAT in its name and ends with .csv
  exp_data <- list.files(path = paste(path_data_folder, file_list[i], sep = ""), pattern = 'EXNAT.*\\.csv$')
  # also set the path to the csv file that contains the questionnaire data:
  questionnaire_data <- paste(path_data_folder, file_list[i], "/", file_list[i], ".csv", sep = "")
  
  # Read in files for current participant:
  subj_df       <- read.csv(paste(path_data_folder, file_list[i], "/", exp_data, sep = ""), sep = ",")
  subj_quest_df <- read.csv(questionnaire_data, sep = ",")
  
  
  # get demographical data from the questionnaire df:
  id             <- subset(subj_quest_df, sender == "ID")$ID # individual code
  message(paste(i, " - Reading in file of participant with ID: ", id, sep = ""))
  
  age            <- subset(subj_quest_df, sender == "demographics")$age # age in years
  gender         <- subset(subj_quest_df, sender == "demographics")$gender # gender in passport
  handedness     <- subset(subj_quest_df, sender == "demographics")$handedness # left- or right-handed?
  native_speaker <- as.logical(subset(subj_quest_df, sender == "demographics")$native_speaker) # native German speaker?
  drugs          <- as.logical(subset(subj_quest_df, sender == "demographics")$drugs)
  alcohol        <- as.logical(subset(subj_quest_df, sender == "demographics")$alcohol)
  education      <- subset(subj_quest_df, sender == "demographics")$education
  medicine       <- as.logical(subset(subj_quest_df, sender == "demographics")$medicine)
  neur_disorder  <- as.logical(subset(subj_quest_df, sender == "demographics")$neur_disorder)
  stroke         <- as.logical(subset(subj_quest_df, sender == "demographics")$stroke)
  psych_disorder <- as.logical(subset(subj_quest_df, sender == "demographics")$psych_disorder)
  reading_weakness <- as.logical(subset(subj_quest_df, sender == "demographics")$reading_weakness)
  seeing_impaired  <- as.logical(subset(subj_quest_df, sender == "demographics")$seeing_impaired)
  colour_vision_impaired <- as.logical(subset(subj_quest_df, sender == "demographics")$colour_vision_impaired)
  hearing_impaired  <- as.logical(subset(subj_quest_df, sender == "demographics")$hearing_impaired)
  CI_or_hearing_aid <- as.logical(subset(subj_quest_df, sender == "demographics")$CI_or_hearing_aid)
  flicker_freq   <-  "15 Hz" # we only used a 15 Hz flicker
  excl           <- FALSE # Should participant be excluded from further analysis? Default: No.

  # Fix small bug in some of the earlier datasets: If education is "undefined", code it as school_abitur.
  # Explanation: I forgot to include a coding for the options "Abitur" and "no education" and fixed this after the first 8 participants or so. I only tested university students, so they obviously have to have Abitur. 
  
  if (education == "undefined"){
    education <- "school_abitur"
  }
  
  
  # CALCULATE AQ/SPQ SCORES
  
  AQ_SS   <- NA
  AQ_AS   <- NA
  AQ_AD   <- NA
  AQ_C    <- NA
  AQ_I    <- NA
  AQ_all  <- NA
  SQP_RI  <- NA
  SQP_SA  <- NA
  SQP_MD  <- NA
  SQP_UW  <- NA 
  SQP_EV  <- NA
  SQP_KEF <- NA
  SQP_US  <- NA
  SQP_EA  <- NA
  SQP_AW  <- NA
  SPQ_all <- NA
  

  # append them to df_demogr:
  df_demogr <- as.data.frame(cbind(id, age, gender,
                                   handedness, native_speaker,
                                   drugs, alcohol, education, medicine,
                                   neur_disorder, stroke, psych_disorder,
                                   reading_weakness, seeing_impaired, colour_vision_impaired,
                                   hearing_impaired, CI_or_hearing_aid,
                                   AQ_SS, AQ_AS, AQ_AD, AQ_C, AQ_I, AQ_all,
                                   SQP_RI, SQP_SA, SQP_MD, SQP_UW, SQP_EV, 
                                   SQP_KEF, SQP_US, SQP_EA, SQP_AW, SPQ_all,
                                   flicker_freq, excl))
  
  ### SPQ: 
  # From the Original Questionnaire: "Alle mit „Ja“ beantworteten Items werden mit 1 verrechnet. 
  #                                   Der Gesamtwert ist die Summe aller Subskalenwerte."
  # So basically all items where participant answered "yes" is counted as 1, "no" is counted as 0. 
  # If you get the sum for each of the scales, you have the sub scale score
  
  # Loop scales in SPQ and count sub scales scores
  
  SPQ_scales <- c("RI",  # Referenzideen (counterintuitively, RI ≠ Rihanna here)
                  "SA",  # (exzessive) soziale Angst
                  "MD",  # Ungwöhnliche Glaubensinhalte / Magisches Denken
                  "UW",  # Ungewöhnliche Wahrnehmungen
                  "EV",  # Ungewöhnliches oder exzentrisches Verhalten
                  "KEF", # Keine engen Freunde
                  "US",  # Ungewöhnliche Sprache
                  "EA",  # Eingeschränkter Affekt
                  "AW")  #Argwohn / Wahnähnliche Vorstellungen
  
  for (curr_subscale in SPQ_scales){
    #print(curr_subscale)
    # get names of all columns that end with the current sub scale name
    curr_columns <- as.vector(names(subj_quest_df)[grep("AW", names(subj_quest_df))])
    # get subset of df where columns have these names:
    curr_columns <- subj_quest_df[curr_columns]
    
    # get only the row where we have values:
    curr_columns <- curr_columns[which(subj_quest_df$sender == "SPQ-G"), ]
    
    # now count how many times participant answered "yes" (label = 1)
    curr_subscale_score <- sum(curr_columns)
    #print(curr_subscale_score)
    
    # put subscale score into demographics df:
    df_demogr[paste("SQP_", curr_subscale, sep = "")] <- as.numeric(curr_subscale_score)
    
  } 
  
  # also assign overall score:
  df_demogr$SPQ_all <- sum(subj_quest_df[which(subj_quest_df$sender == "SPQ-G"), 
                                    as.vector(names(subj_quest_df)[grep("SPQ", names(subj_quest_df))])])
  
  

  ### Autism-Spectrum Quotient (AQ)
  
  #Baron-Cohen, S., Wheelwright, S., Skinner, R., Martin, J., & Clubley, E. (2001). 
  # The autism-spectrum quotient (AQ): Evidence from asperger syndrome/high-functioning autism, 
  # males and females, scientists and mathematicians. Journal of autism and developmental disorders, 31(1), 5-17.
  
  # First, we have to recode some of the item scores:
  # Recode all positive values to 1, recode all negative values to 0:
  
  AQ_df <- subj_quest_df[which(subj_quest_df$sender == "AQ-G"), as.vector(names(subj_quest_df)[grep("AQ", names(subj_quest_df))])]
  AQ_df[1,] <- ifelse(as.vector(unlist(AQ_df)) > 0, 1, 0)
  
  
  # Now we have to change the polarity of some of the items. 
  
  # The scores 1 and 0 for the following items can stay as they are: 
  # 2, 4, 5, 6, 7, 9, 12, 13, 16, 18, 19, 20, 21, 22, 23, 26, 33, 35, 39, 41, 42, 43, 45 und 46 
  
  # The scores 1 and 0 for the following items have to be reversed to 0 and 1: 
  # 1, 3, 8, 10, 11, 14, 15, 17, 24, 25, 27, 28, 29, 30, 31, 32, 34, 36, 37, 38, 40, 44, 47, 48, 49 und 50 
  
  reverse_indices <- c(1, 3, 8, 10, 11, 14, 15, 17, 24, 25, 27, 28, 29, 
                       30, 31, 32, 34, 36, 37, 38, 40, 44, 47, 48, 49, 50)
  
  # reverse the values for the items I specified above
  AQ_df[1, reverse_indices] <- ifelse(AQ_df[1, reverse_indices] == 0, 1, 0)
  
  # Now we're all set for counting the subscale scores. 
  
  # The AQ-50 has 5 subscales. 
  # It's a bit difficult to find them as most sites only report how to get & interpret 
  # the overall score, but I found a key here and the scales make sense 
  # to me if I look at the corresponding items: 
  #     https://novopsych.com.au/assessments/diagnosis/autism-spectrum-quotient/
  
  # Social skill:        items 01,11,13,15,22,36,44,45,47,48
  # Attention switching: items 02,04,10,16,25,32,34,37,43,46
  # Attention to detail: items 05,06,09,12,19,23,28,29,30,49
  # Communication:       items 07,17,18,26,27,31,33,35,38,39
  # Imagination:         items 03,08,14,20,21,24,40,41,42,50
  
  df_demogr$AQ_SS <- sum(AQ_df[ , c("AQ_01", "AQ_11", "AQ_13", 
                                    "AQ_15", "AQ_22", "AQ_36", 
                                    "AQ_44", "AQ_45", "AQ_47", 
                                    "AQ_48")])
  
  df_demogr$AQ_AS <- sum(AQ_df[ , c("AQ_02", "AQ_04", "AQ_10",
                                    "AQ_16", "AQ_25","AQ_32",
                                    "AQ_34","AQ_37","AQ_43",
                                    "AQ_46")])
  
  df_demogr$AQ_AD <- sum(AQ_df[ , c("AQ_05","AQ_06","AQ_09",
                                    "AQ_12","AQ_19","AQ_23",
                                    "AQ_28","AQ_29","AQ_30",
                                    "AQ_49")])
  
  df_demogr$AQ_C <- sum(AQ_df[ , c("AQ_07","AQ_17","AQ_18",
                                   "AQ_26","AQ_27","AQ_31",
                                   "AQ_33","AQ_35","AQ_38",
                                   "AQ_39")])
  
  df_demogr$AQ_I <- sum(AQ_df[ , c("AQ_03","AQ_08","AQ_14",
                                   "AQ_20","AQ_21","AQ_24",
                                   "AQ_40","AQ_41","AQ_42",
                                   "AQ_50")])
  
  # get overall AQ score: 
  df_demogr$AQ_all <- sum(AQ_df[ ,])
  
  # add current participant's data to df with data of all participants:
  df_demogr_all <- as.data.frame(rbind(df_demogr_all, df_demogr))
  
  # remove helper variables to keep things tidy
  rm (age, gender, handedness, native_speaker,
      drugs, alcohol, education, medicine,
      neur_disorder, stroke, psych_disorder,
      reading_weakness, seeing_impaired, colour_vision_impaired,
      hearing_impaired, CI_or_hearing_aid,
      flicker_freq, excl)
  
  # save backup of demographics df in current participant's folder
  write.csv(df_demogr, paste(path_data_folder, file_list[i], "/", file_list[i], "_demographics.csv", sep = ""), row.names = FALSE)
  #--> update this later with info on excluded trials & stuff
  

  
  
  # TO DO: Check if this works.
  # Also save the dataframe of the single participant in their 
  # folder so I can access it later in the EEG analysis.
  
  
  
  
  ###########################
  
  # GET RAW TEXT & N-BACK DATA
  
  # save raw df for later
  subj_df_raw <- subj_df
  
  # get rid of some unimportant columns:
  subj_df <- subj_df[ ,  c("colour", "target", "nback_response", "nback_RT", "duration", 
                           "text_nr", "trial_nr", "word",
                           "block_nr", "block_name", "block_kind", "block_cond",
                           "question", "chosen_ans", "ans_correct",             
                           "vistask_RT_per_letter", "vistask_target", 
                           "participant", "age", "handedness", "gender", 
                           "meas_hearing", "skip_prediction_tendency")]
  
  # delete weird empty rows (I think they sometimes occur after the dual task blocks, 
  # but it doesn't seem like something is missing.)
  subj_df <- subset(subj_df, !is.na(block_nr))

  # name column "block_name" "block_kind" and "participant" "ID" so it matches the EXNAT-1 variables
  names(subj_df)[which(names(subj_df) == "block_name")] <- "block_kind"
  names(subj_df)[which(names(subj_df) == "participant")] <- "ID"
  
  # add excl column:
  subj_df$excl <- FALSE

  # get rid of reading baseline training (that was just a practice block for the participants)
  #subj_df <- subset(subj_df, block_kind != "Reading_Baseline_training")
  
  ###########################
  
  ### ADD NUMBERED BLOCK NAMES ####
  
  # Problem: 
  # In some cases, 3 blocks of the same kind could just directly follow each other, so 
  # there's no way to tell them apart. If we want to compute d-primes by block, though, 
  # it would be nice if they had unique names and I wouldn't have to build 
  # something from the block numbers and block conditions.
  
  # Idea: 
  # Create counters for BL, 1-back and 2-back main blocks, loop rows, 
  # if a new block starts, update counter and add new block label.
  # Keep in mind that we have more than 300 trials in each block 
  # because we also need labels for the question rows.
  
  # first, just copy the "old" block names
  subj_df$block_names_numbered <- subj_df$block_kind
  
  BL_counter <- 1
  oneback_counter <- 0
  twoback_counter <- 0
  previous_block_counter <- NA
  curr_block_trials <- 1 # count trials of current block
  

  for (curr_row_idx in 1:length(subj_df$block_names_numbered)){
  
    #print(curr_bl_name)
    curr_block_name <- subj_df$block_names_numbered[curr_row_idx]
    
    # For BL: 
    if (curr_block_name == "Reading_Baseline_main"){
      # if we already counted more than 300 trials and it's 
      # the first trial again, reset trial counter and add 1 to block counter:
      if (curr_block_trials > 300 & !is.na(subj_df$trial_nr[curr_row_idx]) & subj_df$trial_nr[curr_row_idx] == 1){
        curr_block_trials <- 1
        BL_counter <- BL_counter + 1
        #print(paste(curr_block_name, "_", BL_counter, sep = "") )
      }
      # rename block: 
      subj_df$block_names_numbered[curr_row_idx] <- paste(curr_block_name, "_", BL_counter, sep = "") 
      # go to next trial: 
      curr_block_trials <- curr_block_trials + 1
    
      
    # FOR 1-back:
    } else if (curr_block_name == "1back_dual_main"){
      # if we already counted more than 300 trials and it's 
      # the first trial again, reset trial counter and add 1 to block counter:
      if (curr_block_trials > 300 & !is.na(subj_df$trial_nr[curr_row_idx]) & subj_df$trial_nr[curr_row_idx] == 1){
        curr_block_trials <- 1
        oneback_counter <- oneback_counter + 1
        #print(paste(curr_block_name, "_", oneback_counter, sep = "") )
      }
      # rename block: 
      subj_df$block_names_numbered[curr_row_idx] <- paste(curr_block_name, "_", oneback_counter, sep = "") 
      # go to next trial: 
      curr_block_trials <- curr_block_trials + 1
    
    # FOR 2-back:
    } else if (curr_block_name == "2back_dual_main"){
      # if we already counted more than 300 trials and it's 
      # the first trial again, reset trial counter and add 1 to block counter:
      if (curr_block_trials > 300 & !is.na(subj_df$trial_nr[curr_row_idx]) & subj_df$trial_nr[curr_row_idx] == 1){
        curr_block_trials <- 1
        twoback_counter <- twoback_counter + 1
        #print(paste(curr_block_name, "_", twoback_counter, sep = "") )
      }
      # rename block: 
      subj_df$block_names_numbered[curr_row_idx] <- paste(curr_block_name, "_", twoback_counter, sep = "") 
      # go to next trial: 
      curr_block_trials <- curr_block_trials + 1
    }  
  }
  
  # check if it worked:
  #print(unique(subj_df$block_names_numbered))
  
  
  ###########################
  
  ### COMPUTE READING SPEED ####
  
  # append reading speed column (words / 100 s as in Lea's thesis)
  # reading speed = 100 seconds * 100 ms/s * reading time
  subj_df$reading_speed <- 100 * 1000 / subj_df$duration
  
  ###########################
  
  # GET ADDITIONAL INFORMATION ON THE TEXTS:
  
  ### PUNCTUATION ####
  
  # Edit the texts a bit. Currently, we have words & punctuation
  # mixed up. Would be nice if we had one word column
  # and some others with info on punctuation.
  
  punctuation <- c(rep("", times = length(subj_df$ID)))
  
  # get all .
  punctuation[grep("[.]", subj_df$word)] <- "point"
  # get all ?
  punctuation[grep("[?]", subj_df$word)] <- "question_mark"
  # get all !
  punctuation[grep("[!]", subj_df$word)] <- "exclamation_mark"
  # get all ,
  punctuation[grep("[,]", subj_df$word)] <- "comma"
  # get all ;
  punctuation[grep("[;]", subj_df$word)] <- "semicolon"
  # get all :
  punctuation[grep("[:]", subj_df$word)] <- "colon"
  
  # get all "
  # This is tricky for various reasons:
  
  # 1. there could be quotes directly after or
  # before a point for example
  
  # Idea: Separate column for quotes
  quotes <- c(rep("", times = length(subj_df$ID)))
  
  # 2. I want to differentiate between opening and closing quotes,
  # but they look the same.
  
  # Idea: make them all "opening quotes" and change every second one to
  # "closing_quote". This should do the trick.
  quotes[grep("\"", subj_df$word)] <- "opening_quote"
  opening <- TRUE
  
  # loop list of quotes
  for (x in 1:length(quotes)){
    # get current value in vector "quotes" & current word
    curr_val  <- quotes[x]
    curr_word <- subj_df$word[x]
    
    if (curr_val == "opening_quote" & # if the current value is not empty
        opening == TRUE & # and we set "opening" to TRUE
        grepl("\"[A-Za-z0-9]+\"", curr_word) == FALSE){ # and there are not 2 quotes around the word
      
      # leave it as is, next quote is the closing one
      opening <- FALSE
      
      # if we have a quote, but it has to be a closing one, change it accordingly
    } else if (curr_val == "opening_quote" &
               opening == FALSE &
               grepl("\"[A-Za-z0-9]+\"", curr_word) == FALSE){
      # change value to closing quote
      quotes[x] <- "closing_quote"
      # next quote is the opening one
      opening <- TRUE
      # if it's a quote around a single word, mark as "both"
    } else if (curr_val == "opening_quote" &
               grepl("\"[A-Za-z0-9]+\"", curr_word) == TRUE){
      # change value to both
      quotes[x] <- "both"
    }
  }
  
  # remove all punctuation
  word_single <- gsub('[[:punct:] ]+',' ', subj_df$word)
  # "trim" away spaces before and after words
  word_single <- trimws(word_single, which = c("left"))
  word_single <- trimws(word_single, which = c("right"))
  # put dashes into spaces between words (words like "ice-cream" for example)
  word_single <- gsub(" ", "-", word_single, fixed = TRUE)
  
  # get word length for single words
  word_length_single <- nchar(word_single)
  
  
  ### N-BACK RESPONSES ####
  
  # check if we had an n-back reaction while the word was shown
  # create vector with only FALSEs
  reaction <- c(rep(FALSE, times = length(subj_df$ID)))
  # check where in subj_df we have responses (hit or false alarm) and change value in reaction vector to TRUE at that index
  reaction[which(subj_df$nback_response == "hit" | subj_df$nback_response == "false alarm")] <- TRUE
  
  # do the same for the subj_df_raw dataframe, because we'll need it later
  subj_df_raw$reaction <- FALSE
  # get idx of all rows where n-back response was recorded, mark those as TRUE in the reaction column.
  subj_df_raw[which(subj_df_raw$nback_response == "hit" | subj_df_raw$nback_response == "false alarm"), "reaction"] <- TRUE
  
  # Then get idx of all rows containing n-back responses again, but this time only of
  # slides where sender == word. We're only looking for the dual-task blocks.
  # Subtract 1 from each idx in this list to get all the slides before. Those should be the "fix_word" slides.
  # Mark the reaction in those slides as TRUE, too. We need them later.
  subj_df_raw[which((subj_df_raw$nback_response == "hit" | subj_df_raw$nback_response == "false alarm") & subj_df_raw$sender == "word") - 1, "reaction"] <- TRUE
  
  
  # APPEND NEW COLUMNS
  # append punctuation and quotes column, column with words without punctuation
  # column with length of each the single words and reaction column as
  # additional columns to subj_df:
  subj_df <- data.frame(cbind(subj_df, word_single, punctuation,
                              quotes, word_length_single, reaction))
  
  
  
  ### STOP WORDS, PUNCTUATION, REACTION AND DURATION OF PREVIOUS WORD ####
  
  # Now also get previous punctuation, previous quotes, previous
  # word and previous duration because why not.
  # First, append empty columns to df:
  subj_df[, c("previous_word", "previous_punctuation", "previous_quotes", "previous_duration", "previous_reaction", "stop_word")] <- ""
  
  # get list of German stop_words
  stop_words <- stopwords("de", source = "snowball")
  # snowball = stopwords list based on the Snowball stemmer's word lists.
  # --> https://snowballstem.org/texts/introduction.html
  
  # now loop rows and gradually fill in the empty columns.
  for (idx in 1:length(subj_df$ID)){
    # if it's the first trial of a new block, go to next iteration
    # because in this case we don't have previous data.
    # Also skip the following part if it's a block without text.
    if (subj_df$trial_nr[idx] > 1 && subj_df$word[idx] != ""){
      # fill in the missing data by getting the values from the previous trial:
      subj_df$previous_word[idx]        <- subj_df$word_single[idx-1]
      subj_df$previous_punctuation[idx] <- subj_df$punctuation[idx-1]
      subj_df$previous_quotes[idx]      <- subj_df$quotes[idx-1]
      subj_df$previous_duration[idx]    <- subj_df$duration[idx-1]
      subj_df$previous_reaction[idx]    <- subj_df$reaction[idx-1]
    }
    
    # check if current word is a stop word or not
    if (subj_df$word[idx] != "" && subj_df$word_single[idx] %in% stop_words){
      subj_df$stop_word[idx] <- TRUE
    } else {
      subj_df$stop_word[idx] <- FALSE
    }
  }
  
  
  ### WORD FREQUENCIES & SURPRISAL SCORES ####
  
  # Include word frequencies & surprisal scores for each word
  
  # append "empty" word frequency & surprisal score columns to df
  # to do so, create a vector of column names
  col_names <- c("word_frequency", paste0("surprisal_", c(1, 4, 12, 60)))#,
  # then add "empty" columns to data frame subj_df
  subj_df[, col_names] <- 0
  
  
  
  
  
  
  # TO DO: Change paths so they work with the here package.
  
  
  
  # get word frequency & surprisal scores for each word
  # load word freq df
  word_freqs_df = read.csv(paste(getwd(), "/word frequencies/Word_freqs.csv", sep = ""), sep = ";", header=TRUE)[2:4]
  
  # load surprisal scores / similarity scores df with TS = context chunk size
  surprisal_df = read.csv(paste(getwd(),"/surprisal scores/surprisal_scores_masked_context.csv", sep = ""), sep = ",", header = TRUE)

  # loop individual texts
  for (curr_text_nr in unique(subj_df$text_nr)){

    # in some blocks we don't have texts, so skip those
    # skip BL training & vistask training as well
    if (curr_text_nr != "" & !is.na(curr_text_nr) & curr_text_nr != "None" & curr_text_nr != "reading_bl_training_text" & curr_text_nr != "vis_task_training"){

      # get word frequencies for current text nr
      curr_word_freqs <- subset(word_freqs_df, text_nr == curr_text_nr)$word_frequency
      
      # get surprisal scores:
      curr_surprisals <- subset(surprisal_df, text_nr == curr_text_nr)
      
      # find out where in the subj_df text the text is located and add the word frequencies there. 
      # Take only the first 300 rows because the rest are the questions' rows.

      # assign word frequencies only once
      subj_df[which(subj_df$text_nr == curr_text_nr),]$word_frequency[0:300] <- curr_word_freqs
      # add surprisal scores:
      curr_row <- which(subj_df$text_nr == curr_text_nr & subj_df$word != "")
      subj_df[curr_row, c("surprisal_1", "surprisal_4",
                          "surprisal_12", "surprisal_60")]  <- curr_surprisals[c("surprisal_1", "surprisal_4",
                                                                                 "surprisal_12",  "surprisal_60")]
    }
  }# END loop texts 
  
  
  ###########################
  # GET PERFORMANCE MEASURES:
  
  ### COMPREHENSION QUESTION PERFORMANCE ####

  # We now get all question data for the main blocks
  Q_df <- subset(subj_df, chosen_ans != "" & block_kind %in% c("Reading_Baseline_main", "2back_dual_main", "1back_dual_main"))[,c("question", "chosen_ans", "ans_correct", "text_nr", "block_kind", "ID", "block_nr")]

  # Just look at the MC questions:
  # For now I don't really care about the answer, I only want to know if they chose the correct one or not.
  
  # typecast everything to logical
  Q_df$ans_correct <- as.logical(Q_df$ans_correct)
  
  # get performance in Qs
  Q_df$nr_correct <- c(rep(NA, times = length(Q_df$ID)))
  
  # for each main block, count how many questions were answered correctly.
  for (curr_block in unique(Q_df$block_nr)){
    curr_Q_df <- subset(Q_df, block_nr == curr_block)
    # count how many questions were answered correctly in the current block
    nr_correct <- length(subset(curr_Q_df, ans_correct == TRUE)$ans_correct)
    # append to Q_df & subj_df$Q_nr_correct
    Q_df[which(Q_df$block_nr == curr_block), ]$nr_correct <- nr_correct
  }
  
  # append Q_df to bigger df for all participants
  df_comprehension_Qs <- as.data.frame(rbind(df_comprehension_Qs, Q_df))
  
  # save in participant's folder:
  write.csv(df_comprehension_Qs,
            file = paste(path_data_folder, file_list[i], "/", file_list[i], "_df_comprehension_Qs.csv", sep = ""), 
            row.names = FALSE)

  
  
  ### N-BACK PERFORMANCE ####
  
  # check n-back performance in each block:
  # --> important: don't check by condition, but by block here!
  nback_block_names = c("1back_single_training1", "1back_single_training2", "1back_single_main", "1back_dual_main_1","1back_dual_main_2",
                        "2back_single_training1", "2back_single_training2", "2back_single_main", "2back_dual_main_1", "2back_dual_main_2")
  
  # append empty d-prime column to df:
  subj_df$dprime <- c(rep(NA, times = length(subj_df$word)))
  
  # loop block names:
  for (block_name in nback_block_names){
    
    # check if block name exists in df:
    if (length(which(unique(subj_df$block_names_numbered) == block_name)) > 0){
      # if so, compute d-primes:
      
      # get data for current block
      curr_block <- subset(subj_df, block_names_numbered == block_name)
      
      # remove trials where the RT was way too fast
      # (= participant reacted by accident)
      curr_block <- subset(curr_block, (nback_RT >= 100 | is.na(nback_RT)))
      
      # if there are still some trials left, compute d-prime
      if (length(curr_block$ID) > 10){
        
        # the main blocks are played twice in the experiment, so separate them
        if (length(curr_block$ID) > 400){
          # get first half, compute d-prime & add to bigger df
          d_prime <- get_dprime(curr_block[c(1:300),]$nback_response)
          subj_df[which(subj_df$block_names_numbered == block_name)[c(1:300)],]$dprime <- d_prime
          
          # same procedure for the second half
          d_prime <- get_dprime(curr_block[c(301:600),]$nback_response)
          subj_df[which(subj_df$block_names_numbered == block_name)[c(310:618)],]$dprime <- d_prime
          
          # if there's only one block:
        } else if (length(curr_block$ID) <= 310){
          # do it only once
          d_prime <- get_dprime(curr_block$nback_response)
          subj_df[which(subj_df$block_names_numbered == block_name),]$dprime <- d_prime
        } # END if loop - check if block exists twice
      } # END if loop - check if there are still enough trials left
    } # END if loop - check for block name in df
  }# END loop - compute d-primes
  
  ###########################
  
  # save current participant's data in their folder:
  write.csv(subj_df,
            file = paste(path_data_folder, file_list[i], "/", file_list[i], "_behav_data.csv", sep = ""), 
            row.names = FALSE)
  
  ###########################
  
  ### MARK PARTICIPANTS / TRIALS FOR EXCLUSION:
  
  # append "empty" exclude trial / exclude participant columns to df
  subj_df$excl_trial       <- FALSE
  subj_df$excl_participant <- FALSE
  
  
  # append subj_df chunk to df_text_data where we collect the data of all participants
  df_text_data <- as.data.frame(rbind(df_text_data, subj_df))

  

  # save current participant's df in their data folder:
  write.csv(subj_df, 
            paste(path_data_folder, file_list[i], "/", file_list[i], "_behav_data.csv", sep = ""),
            row.names = FALSE)
  
  message("------------------------")  
}
```
  
# IMPORTANT: DO WE NEED TO DEFINE OUTLIER TRIALS SO WE DON'T EVEN LOOK AT THEM IN THE EEG DATA OR IS THIS IRRELEVANT FOR THE TRF?
