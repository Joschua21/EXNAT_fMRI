# -*- coding: utf-8 -*-
"""GPT_Surprisal_Score_Calculation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ns3xUr1BKbj9yMLUS093Hya2Url-PzFy

import modules
"""

!pip install transformers

import numpy as np
import pandas as pd

import tensorflow as tf
from transformers import AutoTokenizer, AutoModelWithLMHead
import torch

"""### Load Model
We want a GPT-2 model that's trained on German texts: https://huggingface.co/dbmdz/german-gpt2


"""

# download pre-trained German GPT2 model and tokenizer from the Hugging Face model hub
tokenizer = AutoTokenizer.from_pretrained("dbmdz/german-gpt2")
# initialize the model, use end-of-sequence (EOS) token as padding tokens
model = AutoModelWithLMHead.from_pretrained("dbmdz/german-gpt2", pad_token_id = tokenizer.eos_token_id)

# AutoModelWithLMHead = class from the Hugging Face Transformers library
#     Auto: the library will automatically choose the appropriate model architecture based 
#           on the name of the pre-trained model specified in the from_pretrained() call
#     ModelWithLMHead: LM = language modeling; LM head = type of ANN architecture used for LM - it's typically 
#                           added to the top of a pre-trained LM (e.g. GPT-2 or BERT) to fine-tune the model 
#                           for a specific task (e.g. prediction or question answering)

"""###Import CSV with texts used in EXNAT-1 
The csv contains 4 columns: 

word_punct = single words with punctuation

word_no_punct = single words without punctuation (exception: dashes between 2 words)

text_nr = Number of text, text_01 - text_09

trial_nr = Index of word in the text, starting with 1. There are 300 words (aka trials) in each text.


---


*Hint: Make sure you upload the csv in Google Colab before trying to read it in. You can upload the csv by clicking the file icon in the side bar.*
"""

# read in CSV with texts
texts_df = pd.read_csv('/content/Texts_surprisal_scores.csv', sep=";")

"""###Compute Surprisal Scores for each Word

Loop texts in Text_surprisal_scres.csv. For each word, get chunk with x previous words and calculate probability for predicting the actual word in the text. 
"""

""" Compute surprisal scores with different "context" chunk sizes """
# (running this takes a few hours)

# get unique text numbers in texts_df
text_nrs = list(set(texts_df["text_nr"]))
#print("preparing surprisal scores for the following texts:")
#print(text_nrs)

# collect surprisal scores for different chunk sizes here.
# The chunk sizes I use here are completely arbitrary values.
word_surprisal_128 = [] #
word_surprisal_100 = [] #
word_surprisal_64  = [] #
word_surprisal_50  = [] #
word_surprisal_32  = [] #
word_surprisal_16  = [] #
word_surprisal_8   = [] #
word_surprisal_4   = [] #
word_surprisal_3   = [] #
word_surprisal_2   = [] #
word_surprisal_1   = [] #


chunk_size = [128, 100, 64, 50, 32, 16, 8, 4, 3, 2, 1]
chunk_size_lists = [word_surprisal_128, word_surprisal_100, word_surprisal_64, 
                    word_surprisal_50, word_surprisal_32, word_surprisal_16,
                    word_surprisal_8, word_surprisal_4, word_surprisal_3, 
                    word_surprisal_2, word_surprisal_1]

# loop chunk sizes so I get 11 lists with surprisal scores
for chunk_idx in range(0, len(chunk_size)):
    print("start preparing surprisal scores for chunk size = " + str(chunk_size[chunk_idx]))

    """ loop texts """
    for text_nr in text_nrs:
      # get subset of df with current text
      curr_text = texts_df[texts_df["text_nr"] == text_nr]

      """ loop words - for chunk size = x, start at idx = x (aka with the x + 1th word) """

      # for chunk size x, the first x words don't get surprisal scores
      chunk_size_lists[chunk_idx] = chunk_size_lists[chunk_idx] + [None] * chunk_size[chunk_idx] 

      # loop words
      for word_idx in range(chunk_size[chunk_idx], len(curr_text["word_punct"])):
        
        # get x previous words (context chunk)
        previous_words = list(curr_text["word_punct"])[word_idx - chunk_size[chunk_idx] : word_idx]


        # create attention mask, set weights of some of the last words to 0
        
        # --> I only want to use the new words for the prediction. If the current time scale is 10 for example 
        #     and I already computed surprisal scores for time scales 1, 2 and 3, 
        #     those surprisal scores already contain information on the last 3 words, 
        #     so only the other 7 words are what makes the difference for the prediction on time scale 10.
        #     In this case, I would mask the last 3 words (by setting the attention weights of those words/their tokens to 0)
        #     so the model knows there is something at the end of the context chunk, but the 
        #     information from this part can't be used for the prediction. This way, the surprisal 
        #     scores should only reflect the information on the current timescale without the information on all other time scales.

        # first, find out how many words should be masked. 
        # if it's the last time scale, don't mask any words
        if chunk_idx + 1 == len(chunk_size):
          nr_masked_words = 0
        # if it's not the last time scale, mask words we'll also look at when we 
        # compute surprisal scores on the lower time scales.
        else:
          nr_masked_words = chunk_size[chunk_idx + 1]

        # now find out how many tokens the words we want to mask have:
        # generate token IDs for the words we want to mask and check how many there are
        nr_mask_tokens = len( tokenizer.encode(' '.join( previous_words[0:nr_masked_words] )) )
        # print in console
        print("TS", chunk_size[chunk_idx], "| nr of masked words:", len(previous_words[0:nr_masked_words]),"| nr of masked tokens:" , nr_mask_tokens)


        # turn list of previous words into 1 string
        previous_words = ' '.join(previous_words)

        # generate ids for all elements in string
        ids_list = tokenizer.encode(previous_words) # generate token ids for each of the x previous words
        ids_array = np.expand_dims((ids_list), axis = 0) # put the token IDs into an array


        # TO DO: create attention mask: Use default attention mask, but set 
        # weights of all tokens we want to mask to 0 and keep the rest as is








        """ get actual word + its IDs """
        # We should also predict punctuation. 
        # It's not like the words are shown without punctuation on screen.
        actual_word = list(curr_text["word_punct"])[word_idx]

        # Problem: actual word might have multiple token IDs
        # get all IDs for current word
        act_word_id = tokenizer.encode(actual_word) #Output looks somewhat like this: [44, 305, 479, 5283]
        
        """ loop IDs of current word """
        curr_id_probs = []
        for curr_id in act_word_id:

          print("computing probability for ID " +  str(curr_id) + " of word " + actual_word)
          
          # generate probabilities for each possible token being the actual next token
          output = model.generate(torch.tensor(ids_array),
                                  return_dict_in_generate = True, 
                                  output_scores = True, 
                                  #attention_mask = attention_mask, # set attention mask we defined before
                                  max_new_tokens = 1) # set output length here - 1 because I only want 1 token
          # read out probabilities for all IDs
          logits = output.scores[0] # logits = probabilities with range [0,1] transformed to range [inf, -inf]
          probs = tf.nn.softmax(logits) # transform logits back to probabilities

          # get probability for actual ID being the next one & append it to 
          # array with probabilities of all IDs for current word
          curr_id_probs.append(probs.numpy()[0][curr_id]) 

          # append current token ID to list of previous words (if there are any) 
          # reason: The previous parts of the word are part of the context.
          ids_list = np.append(ids_list, curr_id)
          ids_array = np.expand_dims((ids_list), axis = 0) # put the token IDs into an array

        # multiply all probabilities for current word
        act_word_prob = np.prod(curr_id_probs)
        
        # transform probability value into surprisal score (negative log of the probability)
        # neg log = log(1 / x) with x being the value you want to get the neg log of.
        # I use e as a base value for the log here.
        surprisal_score = np.log( 1 / act_word_prob )

        # if surprisal score == Inf, set surprisal score as 100
        if surprisal_score == float('inf'):
          surprisal_score = 100

        # collect surprisal score in array for all surprisal scores
        chunk_size_lists[chunk_idx].append(surprisal_score)
        print("Chunk size = " + str(chunk_size[chunk_idx]) + " - Surprisal score for actual word " + actual_word +" is " + str(surprisal_score) + ".")
        print("Text Nr = " + str(text_nr) + " - Trial Nr = " + str(word_idx))
        print(" --------------- ")




print("finished generating surprisal scores")



""" append new surprisal score columns to texts_df """

surprisal_128 = chunk_size_lists[0]
surprisal_100 = chunk_size_lists[1]
surprisal_64  = chunk_size_lists[2]
surprisal_50  = chunk_size_lists[3]
surprisal_32  = chunk_size_lists[4]
surprisal_16  = chunk_size_lists[5]
surprisal_8   = chunk_size_lists[6]
surprisal_4   = chunk_size_lists[7]
surprisal_3   = chunk_size_lists[8]
surprisal_2   = chunk_size_lists[9]
surprisal_1   = chunk_size_lists[10]

texts_df = texts_df.assign(surprisal_1 = surprisal_1, 
                           surprisal_2 = surprisal_2,
                           surprisal_3 = surprisal_3,
                           surprisal_4 = surprisal_4,
                           surprisal_8 = surprisal_8,
                           surprisal_16 = surprisal_16,
                           surprisal_32 = surprisal_32,
                           surprisal_50 = surprisal_50,
                           surprisal_64 = surprisal_64,
                           surprisal_100 = surprisal_100,
                           surprisal_128 = surprisal_128)

# print first 370 rows of df to check if it looks correct
# (depending on chunk size x, first x values of a text should be NaN)
#print(texts_df.head(370)) 

""" download texts_df as surprisal_scores.csv """
texts_df.to_csv('surprisal_scores.csv', encoding = 'utf-8-sig') 
from google.colab import files
files.download("surprisal_scores.csv")

"""###Estimate entropy for each prediction

Formula from Lea's thesis:

𝑒𝑛𝑡𝑟𝑜𝑝𝑦(𝑝) = − ∑𝑤𝑝+1∈𝑊 ( 𝑃(𝑤𝑝+1|𝑤1, ... , 𝑤𝑝) * log(𝑃(𝑤𝑝+1|𝑤1, ... , 𝑤𝑝)) ) 

---

How I understand this equation:

Entropy = - sum (probability of *next* word given its context * log (probability of *next* word given its context))

I don't quite get what she sums up here though. Do you calculate entropy for individual words or for the whole text? To me, it looks like she calculates it for the whole text.
"""

""" Estimate entropy for all words in a text on a certain time scale """

# I think I can calculate this using the surprisal scores I calculated before

"""Estimate similariy of word with its preceding context

--> correlate vector representation of current word with average vector representation of context

--> higher correlation = higher similarity of word to context
"""

""" Estimate similarity of words in each text for each time scale """
# running this only takes a few minutes :-)

# get unique text numbers in texts_df
text_nrs = list(set(texts_df["text_nr"]))

chunk_size = [128, 100, 64, 50, 32, 16, 8, 4, 3, 2, 1]
# prepare list containing nested empty lists where 
# we can collect the similarity scores
chunk_size_lists = [[], # 128 
                    [], # 100 
                    [], # 64
                    [], # 50 
                    [], # 32 
                    [], # 16 
                    [], # 8  
                    [], # 4
                    [], # 3
                    [], # 2
                    [] # 1
                    ]

# loop chunk sizes so I get 11 lists with surprisal scores
for chunk_idx in range(0, len(chunk_size)):
    print("start preparing similarity scores for chunk size = " + str(chunk_size[chunk_idx]))

    """ loop texts """
    for text_nr in text_nrs:
      # get subset of df with current text
      curr_text = texts_df[texts_df["text_nr"] == text_nr]

      """ loop words - for chunk size = x, start at idx = x (aka with the x + 1th word) """

      # for chunk size x, the first x words don't get similarity scores
      chunk_size_lists[chunk_idx] = chunk_size_lists[chunk_idx] + [None] * chunk_size[chunk_idx] 

      # loop words
      for word_idx in range(chunk_size[chunk_idx], len(curr_text["word_punct"])):
        
        # get x previous words
        previous_words = list(curr_text["word_punct"])[word_idx - chunk_size[chunk_idx] : word_idx]

        # loop previous words
        prev_words_vector = []
        count_added_vecs = 0 # count how many vectors we added - should == length of context chunk
        
        for prev_word in previous_words:

          # convert word into ID(s)
          txt2id = tokenizer.encode(prev_word)

          # Get the word embedding vector for the current word's ID(s)
          vector = model.transformer.wte.weight[txt2id,:]
          vector = vector.detach().numpy()[0] # turn pytorch tensor into numpy array
          
          # get sum of all values in word vector and (if there are any already) 
          # previous word vectors
          if len(prev_words_vector) == 0:
            prev_words_vector = vector
            count_added_vecs = 1
          else:
            prev_words_vector = np.add(prev_words_vector, vector) 
            count_added_vecs = count_added_vecs + 1

        # after loop, divide all scores in word vector by number of word vectors (aka length of context)
        # divide each element in vector by number of vectors we summed up to get mean word vector
        mean_prev_words_vector = prev_words_vector / count_added_vecs

        # get current word 
        curr_word = list(curr_text["word_punct"])[word_idx]

        # convert word into ID(s)
        word_ids = tokenizer.encode(curr_word)

        # Get the word embedding vector for the current word's ID(s)
        curr_word_vector = model.transformer.wte.weight[word_ids,:]
        curr_word_vector = curr_word_vector.detach().numpy()[0]

        # compute correlation of mean_prev_words_vector and curr_word_vector
        similarity = np.corrcoef(curr_word_vector, mean_prev_words_vector)[0][1]

        # collect surprisal score in array for all surprisal scores
        chunk_size_lists[chunk_idx].append(similarity)
        print("Similarity of word " + curr_word + " with " + str(count_added_vecs) + " previous words is " + str(similarity))

print("finished generating similarity scores")



""" append new similarity score columns to texts_df """

similarity_128 = chunk_size_lists[0]
similarity_100 = chunk_size_lists[1]
similarity_64  = chunk_size_lists[2]
similarity_50  = chunk_size_lists[3]
similarity_32  = chunk_size_lists[4]
similarity_16  = chunk_size_lists[5]
similarity_8   = chunk_size_lists[6]
similarity_4   = chunk_size_lists[7]
similarity_3   = chunk_size_lists[8]
similarity_2   = chunk_size_lists[9]
similarity_1   = chunk_size_lists[10]

texts_df = texts_df.assign(similarity_1   = similarity_1, 
                           similarity_2   = similarity_2,
                           similarity_3   = similarity_3,
                           similarity_4   = similarity_4,
                           similarity_8   = similarity_8,
                           similarity_16  = similarity_16,
                           similarity_32  = similarity_32,
                           similarity_50  = similarity_50,
                           similarity_64  = similarity_64,
                           similarity_100 = similarity_100,
                           similarity_128 = similarity_128)

# print first 370 rows of df to check if it looks correct
# (depending on chunk size x, first x values of a text should be NaN)
#print(texts_df.head(370)) 

""" download texts_df as similarity_scores.csv """
texts_df.to_csv('similarity_scores.csv', encoding = 'utf-8-sig') 
from google.colab import files
files.download("similarity_scores.csv")

chunk_size = [128, 100, 64, 50, 32, 16, 8, 4, 3, 2, 1]
print(len(chunk_size))

